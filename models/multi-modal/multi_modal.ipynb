{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cc281315-91db-4484-ad96-7c574fa8f159",
      "metadata": {
        "id": "cc281315-91db-4484-ad96-7c574fa8f159"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.transforms as torchvision_transforms\n",
        "from torchvision.models import resnet18\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from transformers import BertModel, BertTokenizer\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import os\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install wandb --upgrade"
      ],
      "metadata": {
        "id": "qI9pcnmYJ9yd"
      },
      "id": "qI9pcnmYJ9yd",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMDLBwBkKNPT",
        "outputId": "bdcb89cd-0254-427f-fbb0-746df22fdbe1"
      },
      "id": "EMDLBwBkKNPT",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msamuelvasserman\u001b[0m (\u001b[33mllm-news-detector\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = dict(\n",
        "    epochs=2,\n",
        "    learning_rate=0.0001,\n",
        "    batch_size=16,\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    ai_dataset_path=[\"./newsgpt_dataset.csv\", \"./rewritten_AI_data.csv\"],\n",
        "    real_dataset_path=[\"./cnn_dataset.csv\"],\n",
        "    ai_img_dir=\"./newsgpt_images\",\n",
        "    real_img_dir=\"./cnn_images\",\n",
        ")"
      ],
      "metadata": {
        "id": "saE6nrfDNwQY"
      },
      "id": "saE6nrfDNwQY",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 🤖\n",
        "\n"
      ],
      "metadata": {
        "id": "hUwRlR2PVrVr"
      },
      "id": "hUwRlR2PVrVr"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e53852fa-4f81-4b0e-b11e-efe6046a3d62",
      "metadata": {
        "id": "e53852fa-4f81-4b0e-b11e-efe6046a3d62"
      },
      "outputs": [],
      "source": [
        "class MultiModalModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiModalModel, self).__init__()\n",
        "        # Load pre-trained models\n",
        "        self.resnet = resnet18(pretrained=True)\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Freeze the ResNet parameters\n",
        "        for param in self.resnet.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Freeze the BERT parameters\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Define concatonated layers\n",
        "        self.multi_modal_layers = nn.Sequential(\n",
        "            nn.Linear(in_features=self.resnet.fc.out_features + self.bert.config.hidden_size, out_features=512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 1)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, image_inputs, text_inputs):\n",
        "        # Process image input\n",
        "        image_features = self.resnet(image_inputs)\n",
        "        image_features = torch.flatten(image_features, 1)  # Flatten the features\n",
        "\n",
        "        # Process text input\n",
        "        text_features = self.bert(**text_inputs).last_hidden_state[:, 0, :]  # Get the [CLS] token's features\n",
        "\n",
        "        # Concatenate features\n",
        "        combined_features = torch.cat((image_features, text_features), dim=1)\n",
        "\n",
        "        # Pass through additional layers\n",
        "        output = self.multi_modal_layers(combined_features)\n",
        "\n",
        "        output_binary = self.sigmoid(output)\n",
        "\n",
        "        return output_binary"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data 📊\n"
      ],
      "metadata": {
        "id": "E3M4cHURVyXa"
      },
      "id": "E3M4cHURVyXa"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f1eb8c1f-c806-4dac-a6e6-c900a55fb920",
      "metadata": {
        "id": "f1eb8c1f-c806-4dac-a6e6-c900a55fb920"
      },
      "outputs": [],
      "source": [
        "class MultiModalDataset(Dataset):\n",
        "    def __init__(self, dataframe, ai_img_dir, real_img_dir, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.transform = transform\n",
        "        self.text_idx = dataframe.columns.get_loc('Text')\n",
        "        self.title_idx = dataframe.columns.get_loc('Title')\n",
        "        self.image_idx = dataframe.columns.get_loc('Image')\n",
        "        self.label_idx = dataframe.columns.get_loc('Label')\n",
        "        self.ai_img_dir = ai_img_dir\n",
        "        self.real_img_dir = real_img_dir\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        text = self.dataframe.iloc[idx, self.text_idx]\n",
        "        title = self.dataframe.iloc[idx, self.title_idx]\n",
        "        label = self.dataframe.iloc[idx, self.label_idx]\n",
        "\n",
        "        img_folder = self.ai_img_dir if label == 1 else self.real_img_dir\n",
        "        img_name = os.path.join(img_folder, str(self.dataframe.iloc[idx, self.image_idx]))\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return title, text, image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a694d8e7-eed2-46f6-a27f-941861f00d4b",
      "metadata": {
        "id": "a694d8e7-eed2-46f6-a27f-941861f00d4b"
      },
      "outputs": [],
      "source": [
        "class ImageDataset(Dataset):\n",
        "    \"\"\"\n",
        "    create image dataset for loading training images and calculating mean and std of normalization\n",
        "    for image transforms in the MMM\n",
        "\n",
        "    input: dataframe with Image, Label columns\n",
        "    \"\"\"\n",
        "    def __init__(self, dataframe, ai_img_dir, real_img_dir, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.ai_img_dir = ai_img_dir\n",
        "        self.real_img_dir = real_img_dir\n",
        "        self.image_idx = dataframe.columns.get_loc('Image')\n",
        "        self.label_idx = dataframe.columns.get_loc('Label')\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        label = self.dataframe.iloc[idx, self.label_idx]\n",
        "\n",
        "        img_folder = self.ai_img_dir if label == 1 else self.real_img_dir\n",
        "        img_name = os.path.join(img_folder, str(self.dataframe.iloc[idx, self.image_idx]))\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "534961b3-c6b5-4c40-9427-5813f7b72be9",
      "metadata": {
        "id": "534961b3-c6b5-4c40-9427-5813f7b72be9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import transforms as torchvision_transforms\n",
        "\n",
        "def get_mean_std(loader):\n",
        "    # Variables to accumulate the sum and sum of squares\n",
        "    channel_sum, channel_sum_squared, num_batches = 0, 0, 0\n",
        "\n",
        "    for images in loader:\n",
        "        # Assumes images are of shape (batch_size, num_channels, height, width)\n",
        "        channel_sum += torch.mean(images, dim=[0, 2, 3])\n",
        "        channel_sum_squared += torch.mean(images**2, dim=[0, 2, 3])\n",
        "        num_batches += 1\n",
        "\n",
        "    # Calculate the mean and std dev\n",
        "    mean = channel_sum / num_batches\n",
        "    # std = sqrt(E[X^2] - (E[X])^2)\n",
        "    std = (channel_sum_squared / num_batches - mean ** 2) ** 0.5\n",
        "\n",
        "    return mean, std\n",
        "\n",
        "def get_normalization_values(dataframe, ai_img_dir, real_img_dir):\n",
        "    transforms = torchvision_transforms.Compose([\n",
        "        torchvision_transforms.ToTensor(),\n",
        "        torchvision_transforms.Resize((224, 224)),\n",
        "        torchvision_transforms.CenterCrop(224),\n",
        "    ])\n",
        "\n",
        "    # Assuming ImageDataset is defined elsewhere and correctly handles the dataframe and directories\n",
        "    dataset = ImageDataset(dataframe, ai_img_dir, real_img_dir, transforms)\n",
        "\n",
        "    batch_size = 32\n",
        "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    mean, std = get_mean_std(loader)\n",
        "    return mean, std\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "5d46ec01-0e45-4fa7-9374-d7e53530ab92",
      "metadata": {
        "id": "5d46ec01-0e45-4fa7-9374-d7e53530ab92"
      },
      "outputs": [],
      "source": [
        "def get_data(ai_dataset_path, real_dataset_path, ai_img_dir, real_img_dir):\n",
        "\n",
        "    # get the datasets\n",
        "\n",
        "    ai_data = pd.DataFrame()\n",
        "    for dataset in ai_dataset_path:\n",
        "      ai_data = pd.concat([pd.read_csv(dataset), ai_data])\n",
        "\n",
        "    real_data = pd.DataFrame()\n",
        "    for dataset in real_dataset_path:\n",
        "      real_data = pd.concat([pd.read_csv(dataset), real_data])\n",
        "\n",
        "    combined_data = pd.concat([ai_data, real_data])\n",
        "\n",
        "    # shuffle dataset before hand\n",
        "    combined_data_shuffled = combined_data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    mean, std = get_normalization_values(combined_data[['Image', 'Label']], ai_img_dir, real_img_dir)\n",
        "\n",
        "    print(mean, std)\n",
        "\n",
        "    transform = torchvision_transforms.Compose([\n",
        "        torchvision_transforms.Resize(256),\n",
        "        torchvision_transforms.CenterCrop(224),\n",
        "        torchvision_transforms.ToTensor(),\n",
        "        torchvision_transforms.Normalize(mean=mean, std=std),\n",
        "    ])\n",
        "\n",
        "    # create dataset class\n",
        "    dataset = MultiModalDataset(\n",
        "        dataframe=combined_data,\n",
        "        ai_img_dir=ai_img_dir,\n",
        "        real_img_dir=real_img_dir,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    # Split the dataset\n",
        "    train_size = int(0.75 * len(dataset)) #60%\n",
        "    val_size = int(0.10 * len(dataset)) #30%\n",
        "    test_size = len(dataset) - train_size - val_size #10%\n",
        "\n",
        "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training 👟"
      ],
      "metadata": {
        "id": "R-vyl7CxWDtC"
      },
      "id": "R-vyl7CxWDtC"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "77433b81-df67-4983-b558-d57ebed509bb",
      "metadata": {
        "id": "77433b81-df67-4983-b558-d57ebed509bb"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, val_loader, criterion, optimizer, config):\n",
        "\n",
        "    wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
        "\n",
        "    # Run training and track with wandb\n",
        "    total_batches = len(train_loader) * config[\"epochs\"]\n",
        "    example_ct = 0  # number of examples seen\n",
        "    batch_ct = 0\n",
        "    for epoch in tqdm(range(config[\"epochs\"])):\n",
        "        for title, text, image, label in train_loader:\n",
        "            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "            text = tokenizer(text, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=512)\n",
        "\n",
        "            loss = train_batch(text, image, label, model, optimizer, criterion)\n",
        "            example_ct += len(image)\n",
        "            batch_ct += 1\n",
        "\n",
        "            # Report metrics every 3rd batch\n",
        "            if ((batch_ct + 1) % 3) == 0:\n",
        "                train_log(loss, example_ct, epoch)\n",
        "\n",
        "        # run validation every epoch\n",
        "        test(model, val_loader, epoch=epoch)\n",
        "\n",
        "\n",
        "def train_batch(text, image, label, model, optimizer, criterion):\n",
        "    text, image, label = text.to(config[\"device\"]), image.to(config[\"device\"]), label.to(config[\"device\"])\n",
        "\n",
        "    # Forward pass ➡\n",
        "    output = model(image, text)\n",
        "    loss = criterion(torch.squeeze(output, 1), label.float())\n",
        "\n",
        "    # Backward pass ⬅\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Step with optimizer\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e3089c1b-a032-481c-9eb0-c1fc73d4dbd3",
      "metadata": {
        "id": "e3089c1b-a032-481c-9eb0-c1fc73d4dbd3"
      },
      "outputs": [],
      "source": [
        "def train_log(loss, example_ct, epoch):\n",
        "    wandb.log({\"epoch\": epoch, \"loss\": loss}, step=example_ct)\n",
        "    print(f\"Loss after {str(example_ct).zfill(5)} examples: {loss:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing 🧪"
      ],
      "metadata": {
        "id": "t23Vblo9WQvc"
      },
      "id": "t23Vblo9WQvc"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "a56fef7e-f306-45e0-9dec-773ee98904ca",
      "metadata": {
        "id": "a56fef7e-f306-45e0-9dec-773ee98904ca"
      },
      "outputs": [],
      "source": [
        "def test(model, loader, epoch=None):\n",
        "    model.eval()\n",
        "\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "\n",
        "    # Run the model on some test examples\n",
        "    with torch.no_grad():\n",
        "        correct, total = 0, 0\n",
        "        for title, text, image, label in loader:\n",
        "            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "            text = tokenizer(text, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=512)\n",
        "\n",
        "            text, image, label = text.to(config[\"device\"]), image.to(config[\"device\"]), label.to(config[\"device\"])\n",
        "            output = model(image, text)\n",
        "            predicted = (output.data > 0.5).long()\n",
        "            total += label.size(0)\n",
        "            correct += (predicted == label).sum().item()\n",
        "\n",
        "            all_labels.extend(label.cpu().numpy())\n",
        "            flattened_predictions = [pred[0] for pred in predicted.cpu().numpy().tolist()]\n",
        "            all_predictions.extend(flattened_predictions)\n",
        "\n",
        "    print(all_labels, all_predictions)\n",
        "    accuracy = accuracy_score(all_labels, all_predictions)\n",
        "    precision = precision_score(all_labels, all_predictions, zero_division=0)\n",
        "    recall = recall_score(all_labels, all_predictions, zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_predictions, zero_division=0)\n",
        "\n",
        "    print(f\"Accuracy of the model on the {total} test examples: {accuracy:.2%}\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1 Score: {f1:.2f}\")\n",
        "\n",
        "    # Log metrics\n",
        "    if epoch:\n",
        "      wandb.log({\"val_accuracy\": accuracy, \"val_precision\": precision, \"val_recall\": recall, \"val_f1_score\": f1, \"epoch\": epoch})\n",
        "    else:\n",
        "      wandb.log({\"test_accuracy\": accuracy, \"test_precision\": precision, \"test_recall\": recall, \"test_f1_score\": f1})\n",
        "\n",
        "    # Save the model\n",
        "    #wandb.save(\"mmm.onnx\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline 😎"
      ],
      "metadata": {
        "id": "95nkVa9QWTeL"
      },
      "id": "95nkVa9QWTeL"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "586bf741-7bd4-4741-8c50-ad2118b4bfd9",
      "metadata": {
        "id": "586bf741-7bd4-4741-8c50-ad2118b4bfd9"
      },
      "outputs": [],
      "source": [
        "def model_pipeline(config):\n",
        "\n",
        "  with wandb.init(project=\"multi-modal model\", config=config):\n",
        "      # make the model, data, and optimization problem\n",
        "      config = wandb.config\n",
        "\n",
        "      print(config)\n",
        "\n",
        "      model, train_loader, val_loader, test_loader, criterion, optimizer = make(config)\n",
        "      #print(model)\n",
        "\n",
        "      # and use them to train the model\n",
        "      train(model, train_loader, val_loader, criterion, optimizer, config)\n",
        "\n",
        "      # and test its final performance\n",
        "      test(model, test_loader)\n",
        "\n",
        "      torch.save(model.state_dict(), '/content/drive/MyDrive/model_saved.pth')\n",
        "\n",
        "      return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "43b44ff9-c7c2-4fd5-9472-bffd02343fe7",
      "metadata": {
        "id": "43b44ff9-c7c2-4fd5-9472-bffd02343fe7"
      },
      "outputs": [],
      "source": [
        "def make(config):\n",
        "    # Make the data\n",
        "    train, val, test = get_data(\n",
        "        config[\"ai_dataset_path\"],\n",
        "        config[\"real_dataset_path\"],\n",
        "        config[\"ai_img_dir\"],\n",
        "        config[\"real_img_dir\"],\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train, batch_size=config[\"batch_size\"], shuffle=True)\n",
        "    val_loader = DataLoader(val, batch_size=config[\"batch_size\"], shuffle=True)\n",
        "    test_loader = DataLoader(test, batch_size=config[\"batch_size\"], shuffle=True)\n",
        "\n",
        "    # Make the model\n",
        "    model = MultiModalModel().to(config[\"device\"])\n",
        "\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
        "\n",
        "    return model, train_loader, val_loader, test_loader, criterion, optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "249ecb8d-53f3-40e5-b7ab-7a1f526dcdaf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "42c5e0e091eb4afba351718791733603",
            "6405320963064c87984af04a3ce0c7b6",
            "073f0f9801a94772b060ceb76779a52f",
            "0850912bf019483ab2e8e381e7c25581",
            "709a5bd057e648109c56e77fb2df662c",
            "27ee05d170a747fb96bbb93e54711c47",
            "5436048c2eaf4b3e9e948896cba2f259",
            "9b3a0f8c60eb496f88838fc7c998ad7f"
          ]
        },
        "id": "249ecb8d-53f3-40e5-b7ab-7a1f526dcdaf",
        "outputId": "8ec9411f-3534-4b24-8d3f-ccb6836914f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running on cuda\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240330_213212-1ma3400e</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/llm-news-detector/multi-modal%20model/runs/1ma3400e/workspace' target=\"_blank\">genial-frost-56</a></strong> to <a href='https://wandb.ai/llm-news-detector/multi-modal%20model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/llm-news-detector/multi-modal%20model' target=\"_blank\">https://wandb.ai/llm-news-detector/multi-modal%20model</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/llm-news-detector/multi-modal%20model/runs/1ma3400e/workspace' target=\"_blank\">https://wandb.ai/llm-news-detector/multi-modal%20model/runs/1ma3400e/workspace</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epochs': 2, 'learning_rate': 0.0001, 'batch_size': 16, 'device': 'cuda', 'ai_dataset_path': ['./newsgpt_dataset.csv', './rewritten_AI_data.csv'], 'real_dataset_path': ['./cnn_dataset.csv'], 'ai_img_dir': './newsgpt_images', 'real_img_dir': './cnn_images'}\n",
            "                                                 Title  \\\n",
            "0    Miriam Margolyes Expresses Concern Over Adult ...   \n",
            "1    Poseidona: Transforming Invasive Seaweed into ...   \n",
            "2    Trump Considers National Abortion Ban at 16 Weeks   \n",
            "3    Jared Kushner’s $500M Belgrade Hotel Deal: Ech...   \n",
            "4    Global St. Patrick’s Day Celebrations: Dublin,...   \n",
            "..                                                 ...   \n",
            "175  Devastating Tornadoes Strike Eastern Indiana a...   \n",
            "176  Marc Fogel, American Teacher Imprisoned in Rus...   \n",
            "177  Chelsea WSL Team Eyes Historic Quadruple Amids...   \n",
            "178  West Ham’s Victory Boosts England’s Chances fo...   \n",
            "179  Rishi Sunak Faces Voter Backlash, Described as...   \n",
            "\n",
            "                                                  Text  \\\n",
            "0    Miriam Margolyes, the 82-year-old actor who po...   \n",
            "1    Invasive seaweed, known as Rugulopteryx okamur...   \n",
            "2    Former President Donald Trump has recently rev...   \n",
            "3    Jared Kushner, former advisor to ex-President ...   \n",
            "4    St Patrick’s Day was celebrated with gusto acr...   \n",
            "..                                                 ...   \n",
            "175  Severe storms, likely including tornadoes, wre...   \n",
            "176  Marc Fogel, an American teacher who has been i...   \n",
            "177  Chelsea FC’s Women’s Super League (WSL) team i...   \n",
            "178  West Ham’s recent 5-1 aggregate victory over B...   \n",
            "179  Rishi Sunak, the current Prime Minister, has b...   \n",
            "\n",
            "                                                 Image  Label  \n",
            "0    miriam margolyes expresses concern over adult ...      1  \n",
            "1    poseidona transforming invasive seaweed into s...      1  \n",
            "2    trump considers national abortion ban at 16 we...      1  \n",
            "3    jared kushners $500m belgrade hotel deal echoe...      1  \n",
            "4    global st. patricks day celebrations dublin, l...      1  \n",
            "..                                                 ...    ...  \n",
            "175  devastating tornadoes strike eastern indiana a...      1  \n",
            "176  marc fogel, american teacher imprisoned in rus...      1  \n",
            "177  chelsea wsl team eyes historic quadruple amids...      1  \n",
            "178  west hams victory boosts englands chances for ...      1  \n",
            "179  rishi sunak faces voter backlash, described as...      1  \n",
            "\n",
            "[180 rows x 4 columns]\n",
            "                                                 Title  \\\n",
            "0    Miriam Margolyes Expresses Concern Over Adult ...   \n",
            "1    Poseidona: Transforming Invasive Seaweed into ...   \n",
            "2    Trump Considers National Abortion Ban at 16 Weeks   \n",
            "3    Jared Kushner’s $500M Belgrade Hotel Deal: Ech...   \n",
            "4    Global St. Patrick’s Day Celebrations: Dublin,...   \n",
            "..                                                 ...   \n",
            "175  Devastating Tornadoes Strike Eastern Indiana a...   \n",
            "176  Marc Fogel, American Teacher Imprisoned in Rus...   \n",
            "177  Chelsea WSL Team Eyes Historic Quadruple Amids...   \n",
            "178  West Ham’s Victory Boosts England’s Chances fo...   \n",
            "179  Rishi Sunak Faces Voter Backlash, Described as...   \n",
            "\n",
            "                                                  Text  \\\n",
            "0    Miriam Margolyes, the 82-year-old actress know...   \n",
            "1    The global issue of invasive seaweed, Rugulopt...   \n",
            "2    Former President Donald Trump has recently ann...   \n",
            "3    Jared Kushner, the former advisor to ex-Presid...   \n",
            "4    St. Patrick’s Day was joyously celebrated arou...   \n",
            "..                                                 ...   \n",
            "175  A series of severe storms, potentially includi...   \n",
            "176  Marc Fogel, an American teacher who has been h...   \n",
            "177  Chelsea FC’s Women’s Super League (WSL) team i...   \n",
            "178  West Ham's recent dominant 5-1 aggregate trium...   \n",
            "179  Rishi Sunak, the current Prime Minister, has f...   \n",
            "\n",
            "                                                 Image  Label  \n",
            "0    miriam margolyes expresses concern over adult ...      1  \n",
            "1    poseidona transforming invasive seaweed into s...      1  \n",
            "2    trump considers national abortion ban at 16 we...      1  \n",
            "3    jared kushners $500m belgrade hotel deal echoe...      1  \n",
            "4    global st. patricks day celebrations dublin, l...      1  \n",
            "..                                                 ...    ...  \n",
            "175  devastating tornadoes strike eastern indiana a...      1  \n",
            "176  marc fogel, american teacher imprisoned in rus...      1  \n",
            "177  chelsea wsl team eyes historic quadruple amids...      1  \n",
            "178  west hams victory boosts englands chances for ...      1  \n",
            "179  rishi sunak faces voter backlash, described as...      1  \n",
            "\n",
            "[180 rows x 4 columns]\n",
            "                                                 Title  \\\n",
            "0    There's a shortage of truckers, but TuSimple t...   \n",
            "1    Bioservo's robotic 'Ironhand' could protect fa...   \n",
            "2    This swarm of robots gets smarter the more it ...   \n",
            "3    Russia is no longer an option for investors. T...   \n",
            "4    Russian energy investment ban part of new EU s...   \n",
            "..                                                 ...   \n",
            "244  Opinion: Cheesy Christmas movies are just what...   \n",
            "245  Opinion: Best Buy founder: What every US colle...   \n",
            "246  Opinion: It's time to ban Congress members fro...   \n",
            "247  Opinion: Omicron is crushing small businesses ...   \n",
            "248  Opinion: The US is starting to make a manufact...   \n",
            "\n",
            "                                                  Text  \\\n",
            "0     Right now, there's a shortage of truck driver...   \n",
            "1     Working in a factory or warehouse can mean do...   \n",
            "2     In a Hong Kong warehouse, a swarm of autonomo...   \n",
            "3    New York For many years, the world's most popu...   \n",
            "4    The European Union formally approved on Tuesda...   \n",
            "..                                                 ...   \n",
            "244  Bill Carter, a media analyst for CNN, covered ...   \n",
            "245  Richard M. Schulze is the founder and chairman...   \n",
            "246  Mike Levin is the representative for Californi...   \n",
            "247  Rhonda Sideris and her daughter Heleena Sideri...   \n",
            "248  Brian Deese serves as the director of the Whit...   \n",
            "\n",
            "                                                 Image  Label  \n",
            "0    there's a shortage of truckers, but tusimple t...      0  \n",
            "1    bioservo's robotic 'ironhand' could protect fa...      0  \n",
            "2    this swarm of robots gets smarter the more it ...      0  \n",
            "3    russia is no longer an option for investors. t...      0  \n",
            "4    russian energy investment ban part of new eu s...      0  \n",
            "..                                                 ...    ...  \n",
            "244  opinion cheesy christmas movies are just what ...      0  \n",
            "245  opinion best buy founder what every us college...      0  \n",
            "246  opinion it's time to ban congress members from...      0  \n",
            "247  opinion omicron is crushing small businesses l...      0  \n",
            "248  opinion the us is starting to make a manufactu...      0  \n",
            "\n",
            "[249 rows x 4 columns]\n",
            "tensor([0.4703, 0.4341, 0.4217]) tensor([0.2571, 0.2437, 0.2428])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after 00032 examples: 0.711\n",
            "Loss after 00080 examples: 0.682\n",
            "Loss after 00128 examples: 0.550\n",
            "Loss after 00176 examples: 0.509\n",
            "Loss after 00224 examples: 0.525\n",
            "Loss after 00272 examples: 0.453\n",
            "Loss after 00320 examples: 0.556\n",
            "Loss after 00368 examples: 0.548\n",
            "Loss after 00416 examples: 0.441\n",
            "Loss after 00456 examples: 0.380\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 1/2 [00:46<00:46, 46.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0] [1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0]\n",
            "Accuracy of the model on the 60 test examples: 88.33%\n",
            "Precision: 0.93\n",
            "Recall: 0.84\n",
            "F1 Score: 0.89\n",
            "Loss after 00504 examples: 0.306\n",
            "Loss after 00552 examples: 0.293\n",
            "Loss after 00600 examples: 0.272\n",
            "Loss after 00648 examples: 0.192\n",
            "Loss after 00696 examples: 0.159\n",
            "Loss after 00744 examples: 0.213\n",
            "Loss after 00792 examples: 0.234\n",
            "Loss after 00840 examples: 0.192\n",
            "Loss after 00888 examples: 0.114\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [01:30<00:00, 45.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1] [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1]\n",
            "Accuracy of the model on the 60 test examples: 93.33%\n",
            "Precision: 0.94\n",
            "Recall: 0.94\n",
            "F1 Score: 0.94\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1] [1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1]\n",
            "Accuracy of the model on the 93 test examples: 93.55%\n",
            "Precision: 0.97\n",
            "Recall: 0.94\n",
            "F1 Score: 0.95\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "42c5e0e091eb4afba351718791733603"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁█████████</td></tr><tr><td>loss</td><td>██▆▆▆▅▆▆▅▄▃▃▃▂▂▂▂▂▁</td></tr><tr><td>test_accuracy</td><td>▁█</td></tr><tr><td>test_f1_score</td><td>▁█</td></tr><tr><td>test_precision</td><td>▁█</td></tr><tr><td>test_recall</td><td>▁█</td></tr><tr><td>val_accuracy</td><td>▁</td></tr><tr><td>val_f1_score</td><td>▁</td></tr><tr><td>val_precision</td><td>▁</td></tr><tr><td>val_recall</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>loss</td><td>0.1141</td></tr><tr><td>test_accuracy</td><td>0.93548</td></tr><tr><td>test_f1_score</td><td>0.95082</td></tr><tr><td>test_precision</td><td>0.96667</td></tr><tr><td>test_recall</td><td>0.93548</td></tr><tr><td>val_accuracy</td><td>0.93333</td></tr><tr><td>val_f1_score</td><td>0.9375</td></tr><tr><td>val_precision</td><td>0.9375</td></tr><tr><td>val_recall</td><td>0.9375</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">genial-frost-56</strong> at: <a href='https://wandb.ai/llm-news-detector/multi-modal%20model/runs/1ma3400e/workspace' target=\"_blank\">https://wandb.ai/llm-news-detector/multi-modal%20model/runs/1ma3400e/workspace</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240330_213212-1ma3400e/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiModalModel(\n",
              "  (resnet): ResNet(\n",
              "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer4): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "    (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              "  )\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (multi_modal_layers): Sequential(\n",
              "    (0): Linear(in_features=1768, out_features=512, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=512, out_features=1, bias=True)\n",
              "  )\n",
              "  (sigmoid): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "device = torch.device(config[\"device\"])\n",
        "\n",
        "print(\"running on \" + config[\"device\"])\n",
        "\n",
        "model_pipeline(config)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2PIwk9NP-GSu"
      },
      "id": "2PIwk9NP-GSu",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hvP74lhmMY1N"
      },
      "id": "hvP74lhmMY1N",
      "execution_count": 15,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "toc_visible": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "42c5e0e091eb4afba351718791733603": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6405320963064c87984af04a3ce0c7b6",
              "IPY_MODEL_073f0f9801a94772b060ceb76779a52f"
            ],
            "layout": "IPY_MODEL_0850912bf019483ab2e8e381e7c25581"
          }
        },
        "6405320963064c87984af04a3ce0c7b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_709a5bd057e648109c56e77fb2df662c",
            "placeholder": "​",
            "style": "IPY_MODEL_27ee05d170a747fb96bbb93e54711c47",
            "value": "0.488 MB of 0.488 MB uploaded\r"
          }
        },
        "073f0f9801a94772b060ceb76779a52f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5436048c2eaf4b3e9e948896cba2f259",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9b3a0f8c60eb496f88838fc7c998ad7f",
            "value": 1
          }
        },
        "0850912bf019483ab2e8e381e7c25581": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "709a5bd057e648109c56e77fb2df662c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27ee05d170a747fb96bbb93e54711c47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5436048c2eaf4b3e9e948896cba2f259": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b3a0f8c60eb496f88838fc7c998ad7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}