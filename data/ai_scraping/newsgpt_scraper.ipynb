{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "117d86b0-b86c-4c5e-acbb-a90a0cca7665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ef8bc0a6-8110-4de0-b1fa-1b710b145cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"newsgpt_articles_url\": \"https://newsgpt.ai/ai-news/page/\",\n",
    "    \"saved_images_path\": \"./newsgpt_images\",\n",
    "    \"csv_file_path\": \"./newsgpt_dataset.csv\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8cc128bf-3169-41a4-9d28-ed940f5b2550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_filename(filename: str, replace=' ', max_length=255) -> str:\n",
    "    # Normalize Unicode characters to their closest ASCII representation\n",
    "    filename = unicodedata.normalize('NFKD', filename).encode('ascii', 'ignore').decode('ascii')\n",
    "    \n",
    "    # Replace unwanted characters\n",
    "    filename = re.sub(r'[\\/:*?\"<>|\\r\\n\\t]+', replace, filename)\n",
    "    \n",
    "    # Replace spaces with a chosen character (e.g., underscore)\n",
    "    filename = re.sub(r'\\s+', replace, filename).strip(replace)\n",
    "    \n",
    "    # Ensure the filename is not too long\n",
    "    filename = filename[:max_length].rstrip(replace)\n",
    "    \n",
    "    filename = filename.lower()\n",
    "    \n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a85b19-6f78-411d-806c-9f425cfabf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_urls(num_pages: int = 6) -> list[str]:\n",
    "    article_urls = []\n",
    "    base_url = config[\"newsgpt_articles_url\"]\n",
    "    for page in range(1,num_pages+1):\n",
    "        page_url = base_url + str(page)\n",
    "\n",
    "        res = requests.get(page_url)\n",
    "        soup = bs(res.content, features='html.parser')\n",
    "\n",
    "        # get each article tag element\n",
    "        links = soup.find_all('a', class_=\"awb-custom-text-color awb-custom-text-hover-color\")\n",
    "\n",
    "        for link in links:\n",
    "            article_urls.append(link['href'])\n",
    "\n",
    "    return article_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6a8887a2-b78d-4d19-ae65-712a16661e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_article(url: str) -> tuple:\n",
    "    res = requests.get(url)\n",
    "    soup = bs(res.content, features='html.parser')\n",
    "\n",
    "    title = soup.find('h1', class_=\"fusion-title-heading title-heading-left\").text\n",
    "\n",
    "    text_section = soup.find('div', class_=\"fusion-content-tb fusion-content-tb-1\")\n",
    "    text = text_section.find_all('p') if text_section else []\n",
    "    text = [p_tag.text for p_tag in text]\n",
    "    text = ''.join(text)\n",
    "\n",
    "    img_url = soup.find('img', alt=title)['src']\n",
    "    try:\n",
    "        img_res = requests.get(img_url)\n",
    "    except:\n",
    "        print(f\"error retireving img for article: {title}, url: {img_url}, status_code: {img_res.status_code}\")\n",
    "        return None\n",
    "    \n",
    "    img_filename = f\"{sanitize_filename(title)}.jpg\"\n",
    "\n",
    "    os.makedirs(config[\"saved_images_path\"], exist_ok=True)\n",
    "\n",
    "    file_path = os.path.join(config[\"saved_images_path\"], img_filename)\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        with open(file_path, \"wb\") as img_file:\n",
    "            img_file.write(img_res.content)\n",
    "    else:\n",
    "        print(f\"File {img_filename} already exists.\")\n",
    "\n",
    "    return title, text, img_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b92b1d-a1b9-4443-9753-5ea49aa77318",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_urls = get_article_urls()\n",
    "\n",
    "titles = []\n",
    "texts = []\n",
    "img_filenames = []\n",
    "for url in article_urls:\n",
    "    title, text, img_filename = parse_article(url)\n",
    "    titles.append(title)\n",
    "    texts.append(text)\n",
    "    img_filenames.append(img_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "229a431c-51ff-487c-8198-b85a7da737f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgpt_df = pd.DataFrame({\n",
    "    'Title': titles,\n",
    "    'Text': texts,\n",
    "    'Image': img_filenames,\n",
    "    'Label': 1\n",
    "})\n",
    "\n",
    "newsgpt_df.to_csv(config[\"csv_file_path\"], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc92765c-6205-4c52-a4a0-f770aed112f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
